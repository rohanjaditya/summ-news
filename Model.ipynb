{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ee3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, GRU, TimeDistributed, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bb6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b4cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173fa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd464bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2554806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_c = np.array([len(x.split()) for x in df['Content']])\n",
    "len_s = np.array([len(x.split()) for x in df['Summary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ea7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_content = 400\n",
    "max_len_summary = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e4c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[np.where(np.logical_and(len_c<=max_len_content, len_s<=max_len_summary))[0]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee3662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>air frost hit last week caused catastrophic da...</td>\n",
       "      <td>english winemakers warned least half year grap...</td>\n",
       "      <td>_START_ english winemakers warned least half y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>com lindsay lohan make life little harder max ...</td>\n",
       "      <td>lindsay lohan going guest star broke girls pla...</td>\n",
       "      <td>_START_ lindsay lohan going guest star broke g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>researchers found less third experimental clin...</td>\n",
       "      <td>young people cancer scotland fewer clinical tr...</td>\n",
       "      <td>_START_ young people cancer scotland fewer cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4790</th>\n",
       "      <td>real life atlantis sunk coast egypt nearly yea...</td>\n",
       "      <td>city heracleion sunk mediterranean sea years a...</td>\n",
       "      <td>_START_ city heracleion sunk mediterranean sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>david duckenfield also accepted froze afternoo...</td>\n",
       "      <td>hillsborough police match commander agreed fai...</td>\n",
       "      <td>_START_ hillsborough police match commander ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content  \\\n",
       "4787  air frost hit last week caused catastrophic da...   \n",
       "4788  com lindsay lohan make life little harder max ...   \n",
       "4789  researchers found less third experimental clin...   \n",
       "4790  real life atlantis sunk coast egypt nearly yea...   \n",
       "4791  david duckenfield also accepted froze afternoo...   \n",
       "\n",
       "                                                Summary  \\\n",
       "4787  english winemakers warned least half year grap...   \n",
       "4788  lindsay lohan going guest star broke girls pla...   \n",
       "4789  young people cancer scotland fewer clinical tr...   \n",
       "4790  city heracleion sunk mediterranean sea years a...   \n",
       "4791  hillsborough police match commander agreed fai...   \n",
       "\n",
       "                                          Summary_clean  \n",
       "4787  _START_ english winemakers warned least half y...  \n",
       "4788  _START_ lindsay lohan going guest star broke g...  \n",
       "4789  _START_ young people cancer scotland fewer cli...  \n",
       "4790  _START_ city heracleion sunk mediterranean sea...  \n",
       "4791  _START_ hillsborough police match commander ag...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acad6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['Content'].values, \n",
    "                                                    df['Summary_clean'].values, \n",
    "                                                    test_size=0.1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c646873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0 h 0 min 0.71 s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "tokenizer_content = Tokenizer()\n",
    "tokenizer_content.fit_on_texts(x_train);\n",
    "\n",
    "x_train = tokenizer_content.texts_to_sequences(x_train)\n",
    "x_val = tokenizer_content.texts_to_sequences(x_val)\n",
    "x_test = tokenizer_content.texts_to_sequences(x_test)\n",
    "\n",
    "x_train= pad_sequences(x_train,  maxlen=max_len_content, padding='post')\n",
    "x_val = pad_sequences(x_val,  maxlen=max_len_content, padding='post')\n",
    "x_test = pad_sequences(x_test,  maxlen=max_len_content, padding='post')\n",
    "\n",
    "et = time.time()\n",
    "print(\"Time taken: {:d} h {:d} min {:.2f} s\".format(int((et - st)/3600), int(((et - st)%3600)/60), ((et - st)%3600)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfaeddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0 h 0 min 0.12 s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "tokenizer_summary = Tokenizer()\n",
    "tokenizer_summary.fit_on_texts(y_train);\n",
    "\n",
    "y_train = tokenizer_summary.texts_to_sequences(y_train)\n",
    "y_val = tokenizer_summary.texts_to_sequences(y_val)\n",
    "y_test = tokenizer_summary.texts_to_sequences(y_test)\n",
    "\n",
    "y_train= pad_sequences(y_train,  maxlen=max_len_summary, padding='post')\n",
    "y_val = pad_sequences(y_val,  maxlen=max_len_summary, padding='post')\n",
    "y_test = pad_sequences(y_test,  maxlen=max_len_summary, padding='post')\n",
    "\n",
    "et = time.time()\n",
    "print(\"Time taken: {:d} h {:d} min {:.2f} s\".format(int((et - st)/3600), int(((et - st)%3600)/60), ((et - st)%3600)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d3d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_voc = len(tokenizer_content.word_index) + 1\n",
    "y_voc = len(tokenizer_summary.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ab62e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d585ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 500\n",
    "embedding_units = 500\n",
    "\n",
    "encoder_input = Input(shape=(max_len_content,))\n",
    "\n",
    "encoder_embedding = Embedding(x_voc, embedding_units, trainable=True, name=\"encoder_emb\")(encoder_input)\n",
    "\n",
    "encoder_lstm1 = LSTM(lstm_units, return_sequences=True, return_state=True, name=\"encoder_lstm1\")\n",
    "encoder_layer1, state_a1, state_c1 = encoder_lstm1(encoder_embedding)\n",
    "\n",
    "encoder_lstm2 = LSTM(lstm_units, return_sequences=True, return_state=True, name=\"encoder_lstm2\")\n",
    "encoder_layer2, state_a2, state_c2 = encoder_lstm1(encoder_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b207b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77716524",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "decoder_embedding = Embedding(y_voc, embedding_units, trainable=True, name=\"decoder_emb\")\n",
    "decoder_emb_layer = decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_layer, decoder_state_f, decoder_state_b = decoder_lstm(decoder_emb_layer, initial_state=[state_a2, state_c2])\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation=\"softmax\"))\n",
    "decoder_output = decoder_dense(decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "648c42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69309dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "122/122 [==============================] - 25s 147ms/step - loss: 6.4773 - val_loss: 4.9260\n",
      "Epoch 2/2\n",
      "122/122 [==============================] - 17s 143ms/step - loss: 5.8415 - val_loss: 4.8927\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x_train, y_train[:,:-1]], \n",
    "                    y=y_train.reshape(-1, max_len_summary, 1)[:,1:], \n",
    "                    validation_data=([x_val, y_val[:,:-1]], y_val.reshape(-1, max_len_summary, 1)[:,1:]), \n",
    "                    epochs=2, \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb56a4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ba03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=[encoder_input], outputs=[encoder_layer2, state_a2, state_c2])\n",
    "\n",
    "inference_decoder_input = Input(shape=(max_len_content, lstm_units))\n",
    "decoder_input_a2 = Input(shape=(lstm_units,))\n",
    "decoder_input_c2 = Input(shape=(lstm_units,))\n",
    "\n",
    "inference_decoder_emb = decoder_embedding(decoder_input)\n",
    "\n",
    "inference_decoder_layer, state_a2, state_c2 = decoder_lstm(inference_decoder_emb, \n",
    "                                                           initial_state=[decoder_input_a2, decoder_input_a2])\n",
    "\n",
    "inference_decoder_output = decoder_dense(inference_decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9161e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = Model([decoder_input] + [inference_decoder_input, decoder_input_a2, decoder_input_c2], \n",
    "                        [inference_decoder_output] + [state_a2, state_c2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
